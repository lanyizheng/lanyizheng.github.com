# Spark的使用



---

##1.初始化

```
SparkConf conf = new SparkConf().setAppName(appName).setMaster(master);
JavaSparkContext sc = new JavaSparkContext(conf);
```

这里主要是配置app的名字和设置工作的master

##2.RDD的构建

由集合类型构建：

```
List<Integer> data=Arrays.asList(1,2,3,4,5);
JavaRDD<Integer> distData=sc.paralize(data);
```

由外部文件构建：

```
JavaRDD<String> distFile=sc.textFile("file:///Users/lanyizheng/data/input");
```

##3.RDD的操作

###3.1map和reduce的操作

```
JavaRDD<String> lines=sc.textFile("data.txt");
JavaRDD<Integer> lineLengths = lines.map(new Function<String,Integer>(){
    public Integer call(String s){return s.length();}
});
int totalLength = lines.reduce(new Function2<Integer,Integer,Integer>(){
    public Integer call(Integer a,Integer b){
        return a+b;
    }
});
```

###3.2 RDD的元素获取

打印元素：

在单机运行的时候：`rdd.foreach(println)`或者`rdd.map(println)`

在集群模式上不可以。

集群模式：

`rdd.collect().foreach(println)` 因为collect()方法可以取回整个RDD到一台机器上
`rdd.take(100).foreach(println)` 取前100个元素打印

###3.3键值对的操作

```
JavaRDD<String> lines = sc.textFile("data.txt");
JavaPairRDD<String, Integer> pairs = lines.mapToPair(new PairFunction<String,String,Integer>(){
    public scala.Tuple2<String,Integer> call(String v1){
        return new Tuple2<String,Integer>(v1,1);
    }
});
JavaPairRDD<String, Integer> counts = pairs.reduceByKey(new Function2<Integer,Integer,Integer>(){
    public Integer call(Integer a,Integer b){
        return a+b;
    }
});
//JavaPairRDD<String, Integer> pairs = lines.mapToPair(s -> new Tuple2(s, 1));
//JavaPairRDD<String, Integer> counts = pairs.reduceByKey((a, b) -> a + b);

```

###3.4一些常见的操作函数

 - groupByKey([numTasks])  在一个(k,v)的数据集上调用，返回一个(K,Iterable<V>)对的数据集，参数可选
 - reduceByKey(func,[numTasks])  在一个 (K, V)对的数据集上调用时,返回一个 (K, V) 对的数据集
 - aggregateByKey(zeroValue)(seqOp,comOp,[numTasks])  返回一个 (K, U) 对的数据集
 - sortByKey([ascending],[numTasks])  K必须实现Ordered
 - sample(withReplacement, fraction, seed) 根据 fraction 指定的比例,对数据进行采样,可以选择是否用随机数 进行替换,seed 用于指定随机数生成器种子
 - collect() 以数组的形式返回数据集的所有元素
 - count()   返回数据集元素的个数
 - first()  返回数组集的第一个元素
 - take(n)  返回一个由数据集的前n个元素组成的数组
 - takeOrdered(n,[ordering]) 返回一个数据集的前n个元素。
 - saveAsTextFile(path) 将数据集的元素以text file的形式，保存到本地文件系统指定的目录
 - saveAsSequenceFile(path) 将数据集的元素，以Hadoop sequencefile的格式，保存到各种文件系统的指定路径下，包括本地系统，HDFS或者其他hadoop支持的文件系统。
 - countByKey() 只对 (K,V) 类型的 RDD 有效。返回一个 (K,Int) 对的 hashmap
 - foreach(func) 在数据集的每一个元素上,运行 func 函数。

