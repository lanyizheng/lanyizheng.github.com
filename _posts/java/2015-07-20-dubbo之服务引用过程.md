

服务引用是服务的消费方向注册中心订阅服务提供方提供的服务地址后向服务提供方引用服务的过程。

1. 向注册中心获取服务提供方的服务地址列表
2. 向服务提供方引用服务

##宏观流程图

![1](/images/dubbo/1.jpeg)	


ReferenceConfig.get() 会调用 init() 方法

init() 方法调用 createProxy()
具体看 **createProxy()**

- **refprotocol.refer() 产生一个Invoker**
- **proxyFactory.getProxy()产生一个代理 Proxy**

------

##产生一个 Invoker

引用服务前，首先分析 refprotocol.refer() 产生一个Invoker。

ReferenceConfig先需要判断是否是引用本地服务injvm、是否是点对点直连、是否是通过注册中心连接，判断之后进行最重要的一步：服务的引用。 

若有多个提供者提供服务，Dubbo将多个服务Invoker伪装成一个集群Invoker，且这个集群Invoker内部的多个invoker，由Directory完成，Directory与List类似，但由不同，当提供者推送消息过来后，Directory可以动态变化，还可以通过Router路由提供者及LoadBalance根据负载均衡算法选中一个提供者，所以当上层进行Invoker调用时，会是：
Cluster.invoke() --> Directory.list()  --> Router.route() --> LoadBalance.select() --> invoke.invoke();

![1](/images/dubbo/2.png)


创建一个 Invoker 的过程中，是由 refprotocol.refer(interfaceClass, url)完成，经过很多调用，来到 RegistryProtocol.doRefer() 方法。

1.  根据配置的协议，获得那个注册中心的实例，如 ZooKeeperRegistry 实例
2.  consumer 本身会向注册中心注册
针对zookeeper，就在该服务的路径下创建一个节点 consumers/临时节点
3.  consumer 向注册中心订阅当前需要引用的那个服务
针对zookeeper
在该路径下创建节点 providers，并且观察该节点的子节点列表**注意：这时候会构建调用链，具体调用在 toInvokers() 的代码中**
在该路径下创建节点 configurators，并且观察该节点的子节点列表
在该路径下创建节点 routers，并且观察该节点的子节点列表
4. 调用cluster.join() 方法返回一个 Invoker


调用 loadRegistries(false) 获取信息 包括使用什么注册中心
![1](/images/dubbo/3.png)

调用 **ProtocolFilterWrapper.refer(interfaceClass, url)** 产生一个 Invoker
具体是调用 ProtocolFilterWrapper.refer()  产生一个 Invoker
判断：协议是 registry，调用 ProtocolListenerWrapper.refer() 产生一个 Invoker

**ProtocolListenerWrapper.refer() **
判断：协议是registry，调用 RegistryProtocol.refer()  产生一个 Invoker

经过这两步装饰，进入到 RegistryProtocol。

**RegistryProtocol.refer()**
- 通过registryFactory 获取到 registry （ZookeeperRegistry）
- 消费者也首先需要连接注册中心，即引用注册中心这个特殊的RPC服务，并通过注册中心进行服务的订阅与注册。
- 然后判断引用是否是注册中心RegistryService服务，若是绕过注册中心与集群等直接返回刚得到注册中心服务即可；若不是，则说明是普通服务，则需要进入注册中心与集群下面的逻辑。在选择集群策略前，先需要判断引用服务是否需要合并不同实现的返回结果，及是否配置group="*" merger="true" （配置详细说明再说），若配置了，则选择默认的分组聚合集群策略，若没配置，则选择配置的集群策略cluster="failback"或默认策略。
- 调用 doRefer() 返回一个 Invoker
![1](/images/dubbo/4.png)

**RegistryProtocol.doRefer()**
消费端引用服务主要逻辑部分。
- **组合url、type创建目录类 RegistryDirectory。此类非常重要**，它的功能很复杂，可以看成是NotifyListener服务通知监听器、可以看成Protocol、Registry的聚合类、可以看成一个消费端的List，但它又不同于List，**它可以随着注册中心的消息推送而动态变化服务的Invoker数，时刻监听着提供者的变化，典型观察者模式的使用，它封装了所有服务真正引用逻辑、覆盖配置、路由规则等逻辑**，初始化时只需要向注册中心发起订阅请求，其他逻辑均是**异步**的处理，(***疑问：哪里看出来是异步处理了？--> 回答：这个异步说 zkClient 完成了。***)包括服务引用等，且对上层Cluster层是透明的，ResitryProtocol将多个Invoker伪装成一个Invoker返回给上层来转化成代理，上层甚至不知道这是个集群Invoker，在伪装成一个Invoker时，Cluster也被装饰了，增强了Mock功能
- 调用 registry.register()创建消费者节点
- 调用 RegistryDirectory.subscribe() 订阅
想注册中心订阅服务的时候，传入自己，当关注的服务发生变化时，就会回调方法：notify() 。触发Directory监听器的可以是订阅请求、覆盖策略消息、路由策略消息(后两者由管理页面设置)。
![1](/images/dubbo/5.png)
notify() 最后会调用 RegistryDirectory.toInvokers() 生成调用链。
- 调用cluster.join()，返回一个 Invoker 

**RegistryDirectory.subscribe()**
调用 ZookeeperRegistry.subscribe() 

**ZookeeperRegistry.subscribe()**
(其实是调用父类 FailbackRegistry 的方法)
调用 doSubscribe()

**ZookeeperRegistry.doSubscribe()** 
![1](/images/dubbo/5.png)
调用ZkclientZookeeperClient 的方法 create()
(实际上调用的是父类 AbstractZookeeperClient create()的的方法)
针对zookeeper，这里涉及到3个永久节点：
/dubbo/interfaceA/consumers
/dubbo/interfaceA/configuratiors
/dubbo/interfaceA/routers
![1](/images/dubbo/6.png)
然后调用 notify() 方法

**ZookeeperRegistry.notify()**
这个方法其实不是 ZookeeperRegistry 的，而是它的父类 FailbackRegistry 的
它要调用了 doNotify() 方法

**FailbackRegistry.doNotify()**
它又调用了父类的方法 AbstractRegistry.notify() 方法

**AbstractRegistry.notify()**
3次调用 RegistryDirectory.notify()
（3次：consumers、configurators、routers）

**RegistryDirectory.notify()**
根据调用方就可以知道，分成3种：
1. toConfigurators()
toConfigurators() 方法，这个方法又调用 refreshInvoker()
2. toRouters()
3.  providers() **生成调用链**
4.  最后都会执行 refreshInvoker() 
这些方法调用很深，返回到 RegistryProtocol.doRefer()，然后调用 cluster.join()，这个方法是返回一个 Invoker 的
实际调用的是 MockClusterWrapper.join() 方法，这个方法内部用到了FailoverCluster.join() 然后返回了一个 MockClusterInvoker 的 Invoker 对象

**RegistryDirectory.refreshInvoker() --> RegistryDirectory.toInvokers()**
通过这些方法引用服务，而 toInvokers() 实际调用 DubboProtocol.refer() 来产生 Inovker ， 并构建生成调用链

**DubboProtocol.refer()**
可以看出Invoker聚合了Client。
![1](/images/dubbo/7.png)

**getClients(url)**
此代码负责NIO框架Client创建及初始化，和提供者初始化相似，消费者的初始化也基本上围绕着Client、Handler、Channel对象的创建与不断装饰的过程，不同的是消费者底层是与提供者Server建立连接，此过程在remote层下完成，remote层可分为消息交换层与网路传输层即Exchanger与Transporter层，还有，我们在说提供者初始化时，说过同个JVM中相同协议的服务共享一个Server，同样在消费者初始化时，**引用同一个提供者的所有服务可以共享一个Client进行通信，这也就实现了Server-Client在同一个通道中进行通信，实现长连接的高效通信**，但是在服务请求数据量比较大时或请求数比较多时，可以设置每服务每连接或每服务多连接可以提高通信效率，具体是通过消费者方connections=2设置连接数。所有消费者端Client有两种，一种是共享型Client，一种是创建型Client，当然共享型Client属于创建型Client一部分，下面具体说说这两种Client创建的细节，也是服务引用的重要细节。
![1](/images/dubbo/8.png)

**getSharedClient(url)**
此为创建共享型Client，共享型Client是指消费者引用同一提供者的服务时，使用同一个Client来提高通信效率，所以对于消费者来说，它连接的**每个提供者都需要创建一个创建型Client**，其他来自相同提供者的服务引用即可共享此Client。对于共享型Client虽然可以提高通信效率，但会带来一个问题，就是如果所有共享一个Client的服务中的某个服务close了，Client会导致其他服务都不能继续通信，这是个安全性问题，Dubbo的解决方案是通过计数的方式来控制这个安全问题，并在共享Client真正关闭后创建一个懒加载的幽灵Client，以备特殊情况使用。
![1](/images/dubbo/9.png)

**initClient()**
**创建创建型Client，也是消费者端服务引用最核心的地方**，封装了服务引用中remote层初始化的所有逻辑，与提供者端类似，就是Client、Handler、Channel的创建与不断装饰的过程，在说Client创建细节前，先说说Client类型，与其他产品一样，Dubbo中也有懒加载的概念，即Client分为正常马上连接的Client与懒加载的Client，懒加载的Client是个Client代理，当消费者真正调用服务时，才会去初始化Client、Handler、Channel，也就是才会与Server建立连接。 

Client、Handler、Channel初始化 
此步骤是服务引用的核心的核心，逻辑完全封装在Protocol远程调用层，对于上层是完全透明的。在进行创建前，**Dubbo默认开启remote层的心跳机制与禁止BIO通信，心跳机制是提供者与消费者之间的心跳通信，主要是防止通信异常，如提供者宕机，消费者自动重连提供者。**
服务引用即Client的创建，此过程完成由Directory发起。同提供者类似，服务引用无非也就是初始化信息交换层与网络传输层即Exchanger与Transporter，且分别由门面类Exchangers与Transporters发起，(Exchangers.connect() 和 Transporters.connect() )Transporter返回的Client再经过Exchanger封装装饰后完成初始化并返回给上层Protocol。  
![1](/images/dubbo/10.png)
先初始化 Transports，返回NettyClient
**返回 NettyClient 之前（也就是 New NettyClient 时），会执行 doOpen() 和 connect() 方法，此时已经和 Server 建立连接了！**
![1](/images/dubbo/11.png)
Transports.connect() 调用 NettyTransporter.connect(url, listener)，这个listener是被装饰了的 DecodeHandler
再初始化 Exchangers，返回 HeaderExchangeServer，开启一个心跳线程
![1](/images/dubbo/12.png)

handler 的装饰：
requestHandler  -->  HeaderExchangerHandler  -->   DecodeHandler  -- >  AllChannelHandler  -->  HeartbeatHandler  -->  MultiMessageHandler 
最终返回 NettyClient，再将 NettyClient 包装成 HeaderExchangeClient

到此，返回到 Invoker 的初始化处，所以生成的 Invoker 已经和所有的 服务提供者建立了 **tcp 长连接**，并且还保持了1分钟的心跳。这样也可以看出，这个 Invoker 是一个重对象，因此进行了缓存。

------

##产生一个代理

然后各种返回到 ReferenceConfig 的 createProxy() 方法
![1](/images/dubbo/13.png)

最后进入到 StubProxyFactoryWrapper.getProxy() 返回一个代理 
它调用了 JavassistProxyFactory.getProxy() 返回，它实际上调用了其父类的 AbstractProxyFactory.getProxy() 它又调用了 JavassistProxyFactory.getProxy() 返回

调用 Proxy.getProxy() 返回代理，这个方法奇怪。。。

**Proxy.getProxy()**
主要有2个class生成，用javassist，
- ccp
ccp 继承了 需要代理 的那的接口，并实现了其所有方法的代码，里面用了适配器模式，调用 handler.invoke() 方法。后面可以知道，其实这个 handler 是 InvokerInvocationHandler。
![1](/images/dubbo/14.png)
- ccm
这一步我要吐槽一下dubbo了，它的作用仅是 new 一个对象，而这个对象的newInstance 方法才是真是的生成一个代理。
![1](/images/dubbo/15.png)
![1](/images/dubbo/16.png)
这个newInstance 才会生成 ccp 的实例。

------

## 服务的调用

**InvokerInvocationHandler.invoke()**
![1](/images/dubbo/17.png)
这里的 invoker 对象是前面构建的调用链的对象，类似于一个职责链模式吧

**MockClusterInvoker.invoke()**
判断是否需要mock，不要要，则 FailoverClusterInvoker.invoke()

**FailoverClusterInvoker.invoke()**
根据负载均衡算法，选择一个Invoker
![1](/images/dubbo/18.png)

之后根据各种filter，返回结果 


