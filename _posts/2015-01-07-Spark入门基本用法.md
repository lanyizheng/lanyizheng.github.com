# Spark
Spark是通用的集群计算框架，解决分布式计算框架的两个问题：如何分发数据和如何分发计算。
RDD(resilient distributed datasets,RDDs).


##核心组件

```
1.Spark Core:包含Spark的基本功能
2.Spark SQL:提供hql与Spark进行交互的API，每一个数据库表被当作一个RDD.
3.Spark Streaming:允许对实时数据进行处理和控制。
4.MLib:一个常用机器学习的算法库：算法被实现为对RDD的Spark的操作。
5.GraphX:控制图、并行图操作和计算的一组算法和工具的集合。
``` 

##对Spark编程
代码写入一个惰性求值的驱动程序中，通过一个动作，驱动代码被分发到集群中，各个RDD分区上的worker来执行，然后结果会被发送回驱动程序进行聚合或者编译。
包含以下几个步骤：
1.定义一个或多个RDD，可以通过获取磁盘上的数据，并行化内存中的某些集合，转换成一个已存在的RDD，或者缓存或保存。
2.通过传递一个闭包（函数）给RDD的每一个元素来调用RDD上的操作。
3.使用结果RDD的动作（action）
##spark 启动与使用
spark的python shell启动：
`./bin/pyspark --conf spark.ui.port=3354`
本地启动两个线程:
`./bin/pyspark --master local[2]`
spark默认从hdfs读取数据，可以通过指定file://来指定本地文件系统

避免输出信息太多繁杂，将日志级别修改成WARN等级：
`log4j.rootCategory=WARN, console`

运行一个应用:

`./bin/spark-submit --conf spark.ui.port=3354 hello.py`

hello.py代码如下：

```
from pyspark import SparkContext,SparkConf
conf=SparkConf().setMaster("local").setAppName("My App")
sc=SparkContext(conf=conf)
lines=sc.textFile("hello.py")
pythonLines=lines.filter(lambda line:"Python" in line)
print pythonLines.first()
print pythonLines.first()
print "Hello Spark!"
```
##Spark Hive
下面是Spark访问hive的一个例子：

```python
from pyspark import SparkContext,SparkConf
from pyspark.sql import HiveContext
hiveContext=HiveContext(sc)
hiveContext.sql("use st")
#sql查询返回一个RDD
results=hiveContext.sql("select * from test")
for item in results.collect():
	print item
```
##RDD
spark目标是基于工作集的应用（多个并行操作重用中间结果的应用）提供抽象，同时保持MapReduce及相关模型的优势特性，即自动容错、位置感知性调度和伸缩性。
##分布式SQL 引擎
Spark SQL 可以使用jdbc/odbc或者命令行界面，作为分布式查询引擎。在这种模式下，终端用户或者应用可以直接与Spark SQL交互去执行SQL查询，不需要写任何代码。
###运行Thrift jdbc/odbc 服务器
这里实现的Thrift JDBC/ODBC server对应的HiveServer2。你可以通过beeline脚本使用JDBC server 。
启动JDBC/ODBC server:

```./sbin/start-thriftserver.sh ```

可以通过--hiveconf配置hive属性。

```bash
./sbin/start-thriftserver.sh 
  --hiveconf hive.server2.thrift.port=<listening-port> 
  --hiveconf hive.server2.thrift.bind.host=<listening-host> 
  --master <master-uri>
  ...
```
现在可以使用beeline测试Thrift JDBC/ODBC server:
```./bin/beeline```
在beeline中连接JDBC/ODBC server：
```beeline> !connect jdbc:hive2://localhost:10000```
###运行Spark SQL CLI
Spark Sql CLI是一个方便在本地模式下运行Hive元存储服务并执行查询的一个工具。
启动 Spark SQL CLI,
```
./bin/spark-sql
```

###Spark python的用法

```python
>>>textFile=sc.textFile("file:///opt/spark/README.md")
>>>textFile.count()  #统计RDD文本行数
>>>textFile.first()  #读取第一行内容
>>>textFile.filter(lambda line:"Spark" in line).count() #统计多少行包含了“spark”
>>>textFile.map(lambda line:len(line.split())).reduce(lambda a,b:a if(a>b) else b)
>>>
>>>
>>>wordCounts=textFile.flatMap(lambda line:line.split()).map(limbda word:(word,1)).reduceByKey(lambda a,b:a+b)
>>>wordCounts.collect()
```
独立的应用程序
新建一个SimpleApp.py文件，文件具体内容如下

```python
from pyspark import SparkContext

logFile="file:///opt/spark/README.md"
sc=SparkContext("local","Simple App")
logData=sc.textFile(logFile).cache()

numAs=logData.filter(lambda s:'a' in s).count()
numBs=logData.filter(lambda s:'b' in s).count()
print "Lines with a:%i,lines with b:%i"%(numAs,numBs)
```
提交任务：
`/opt/spark/bin/spark-submit --master local[4] SimpleApp.py`


